{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import email\n",
    "from generate_data import generate_train_and_test_data\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### these are the 10 people whose writing styles we will analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter_list=['kay.mann@enron.com','vince.kaminski@enron.com','jeff.dasovich@enron.com',\n",
    "#                 'chris.germany@enron.com','sara.shackleton@enron.com','tana.jones@enron.com',\n",
    "#                'eric.bass@enron.com','matthew.lenhart@enron.com','kate.symes@enron.com','sally.beck@enron.com']\n",
    "filter_list=['kay.mann@enron.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the enron emails often contain forwarded messages, or the orginal text in a reply to email. we remove all this data since we are trying to understand a particular person's writing style.\n",
    "#### we also split the data into train and test. the test data is what we will try and predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df_train,df_test=generate_train_and_test_data('data/emails.csv',filter_list,min_words=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>From</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kay.mann@enron.com</th>\n",
       "      <td>35.149455</td>\n",
       "      <td>35.082358</td>\n",
       "      <td>35.752754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          All      Train       Test\n",
       "From                                               \n",
       "kay.mann@enron.com  35.149455  35.082358  35.752754"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data=df.pivot_table(index=\"From\",values='NumWords',aggfunc=\"mean\")\n",
    "train_data=df_train.pivot_table(index=\"From\",values='NumWords',aggfunc='mean')\n",
    "test_data=df_test.pivot_table(index=\"From\",values='NumWords',aggfunc='mean')\n",
    "df_summary=all_data.join(train_data,lsuffix='All',rsuffix='Train')\n",
    "df_summary=df_summary.join(test_data,lsuffix='',rsuffix='Test')\n",
    "df_summary.columns=['All','Train','Test']\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>From</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kay.mann@enron.com</th>\n",
       "      <td>202.101188</td>\n",
       "      <td>201.37939</td>\n",
       "      <td>208.591187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           All      Train        Test\n",
       "From                                                 \n",
       "kay.mann@enron.com  202.101188  201.37939  208.591187"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data=df.pivot_table(index=\"From\",values='MessageLength',aggfunc=\"mean\")\n",
    "train_data=df_train.pivot_table(index=\"From\",values='MessageLength',aggfunc='mean')\n",
    "test_data=df_test.pivot_table(index=\"From\",values='MessageLength',aggfunc='mean')\n",
    "df_summary=all_data.join(train_data,lsuffix='All',rsuffix='Train')\n",
    "df_summary=df_summary.join(test_data,lsuffix='',rsuffix='Test')\n",
    "df_summary.columns=['All','Train','Test']\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### take all the text in the training set, and count the total number of words\n",
    "#### then take the text for each author and count the total number of words by author\n",
    "#### finally we take the n_most_frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310134"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "#this is the most frequent count\n",
    "n_most_frequent=50\n",
    "\n",
    "#define a dictionary to hold the word count for each author\n",
    "author_subcorpus_count={}\n",
    "for item in filter_list:\n",
    "    author_subcorpus_count[item]=defaultdict(int)\n",
    "\n",
    "\n",
    "all_text=df_train['FormattedMessage'].tolist()\n",
    "from_list=df_train['From'].tolist()\n",
    "\n",
    "#word counts for the combined corpus\n",
    "word_counts=defaultdict(int)\n",
    "\n",
    "#go through the entire corpus, count words for the combined corpus and for each author\n",
    "for i,text in enumerate(all_text):\n",
    "    sentences=sent_tokenize(text.lower())\n",
    "    for sentence in sentences:\n",
    "        words=word_tokenize(sentence)\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        for word in fdist:\n",
    "            word_counts[word]+=fdist[word]\n",
    "            author_subcorpus_count[from_list[i]][word]+=fdist[word]\n",
    "            \n",
    "#create a list of most frequent words\n",
    "#also check what total word count is (to validate data)\n",
    "freq_list=[]\n",
    "i=0\n",
    "totalWords=0\n",
    "for w in word_counts:\n",
    "    totalWords+=word_counts[w]\n",
    "for w in sorted(word_counts, key=word_counts.get, reverse=True):\n",
    "    if i<n_most_frequent: freq_list.append((w,word_counts[w]))\n",
    "    i+=1\n",
    "totalWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 16543),\n",
       " (',', 13414),\n",
       " ('the', 12620),\n",
       " ('i', 9368),\n",
       " ('to', 8849),\n",
       " ('a', 4945),\n",
       " ('of', 4383),\n",
       " ('kay', 4313),\n",
       " ('and', 4142),\n",
       " ('you', 3851),\n",
       " ('is', 3729),\n",
       " ('it', 3291),\n",
       " ('in', 3141),\n",
       " ('for', 2963),\n",
       " ('?', 2929),\n",
       " ('we', 2605),\n",
       " ('that', 2578),\n",
       " ('be', 2572),\n",
       " ('have', 2513),\n",
       " ('this', 2489),\n",
       " ('on', 2365),\n",
       " (\"n't\", 2150),\n",
       " ('with', 2096),\n",
       " (')', 1933),\n",
       " ('(', 1917),\n",
       " ('thanks', 1703),\n",
       " ('will', 1594),\n",
       " (\"'s\", 1592),\n",
       " ('if', 1555),\n",
       " ('do', 1526),\n",
       " ('are', 1454),\n",
       " (\"'m\", 1392),\n",
       " ('as', 1364),\n",
       " ('me', 1360),\n",
       " ('at', 1219),\n",
       " ('or', 1215),\n",
       " ('can', 1158),\n",
       " ('--', 1149),\n",
       " ('would', 1112),\n",
       " ('so', 1081),\n",
       " ('but', 1071),\n",
       " ('from', 1036),\n",
       " ('not', 985),\n",
       " ('!', 951),\n",
       " ('know', 940),\n",
       " ('my', 934),\n",
       " ('please', 885),\n",
       " ('was', 878),\n",
       " ('get', 851),\n",
       " ('hi', 848)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310134\n"
     ]
    }
   ],
   "source": [
    "#aggregate total words by author\n",
    "#ensure it adds up to total words by corpus\n",
    "totalWordsByAuthor={}\n",
    "totalWords=0\n",
    "for author in author_subcorpus_count:\n",
    "    totalWordsByAuthor[author]=sum(author_subcorpus_count[author][x] for x in author_subcorpus_count[author])\n",
    "    totalWords+=totalWordsByAuthor[author]\n",
    "\n",
    "#we compute the mean for the corpus 2 ways\n",
    "#by corpus - so for example count \"the\" in the entire corpus/ total words in corpus\n",
    "#or compute the prob of \"the\" in each author's corpus and average it\n",
    "#the 2 results are not that different\n",
    "frequentWordsCorpusMean={}\n",
    "frequentWordsCorpusStdDev={}\n",
    "for word,count in freq_list:\n",
    "    frequentWordsCorpusMean[word]=(count+0.000001)/totalWords\n",
    "    frequentWordsCorpusStdDev[word]=0.0\n",
    "\n",
    "\n",
    "topWordsByAuthor={}\n",
    "for item in author_subcorpus_count:\n",
    "    topWordsByAuthor[item]={}\n",
    "    for word,count in freq_list:\n",
    "        wc=author_subcorpus_count[item][word]\n",
    "        wp=(wc+0.000001)/totalWordsByAuthor[item]\n",
    "        topWordsByAuthor[item][word]=wp\n",
    "        \n",
    "frequentWordsMean={}\n",
    "for word,count in freq_list:\n",
    "    frequentWordsMean[word]=0.0\n",
    "    for author in topWordsByAuthor:\n",
    "        frequentWordsMean[word]+=topWordsByAuthor[author][word]\n",
    "    frequentWordsMean[word]/=len(topWordsByAuthor)\n",
    "\n",
    "for word,count in freq_list:\n",
    "    for author in topWordsByAuthor:\n",
    "        diff=topWordsByAuthor[author][word]-frequentWordsCorpusMean[word]\n",
    "        frequentWordsCorpusStdDev[word]+=diff*diff\n",
    "    frequentWordsCorpusStdDev[word]/=len(topWordsByAuthor)\n",
    "    frequentWordsCorpusStdDev[word]=math.sqrt(frequentWordsCorpusStdDev[word])\n",
    "    \n",
    "#print(frequentWordsCorpusMean)\n",
    "print(totalWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-aa577baa5877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mzScoresByAuthor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfrequentWordsCorpusMean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mzScoresByAuthor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopWordsByAuthor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mfrequentWordsCorpusMean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfrequentWordsCorpusStdDev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "#calculate zscores\n",
    "#for each author, calculate the zscore for each of the common words\n",
    "zScoresByAuthor={}\n",
    "for author in topWordsByAuthor:\n",
    "    zScoresByAuthor[author]={}\n",
    "    for word in frequentWordsCorpusMean:\n",
    "        zScoresByAuthor[author][word]=(topWordsByAuthor[author][word]-frequentWordsCorpusMean[word])/frequentWordsCorpusStdDev[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function calculates the zscore for the test text\n",
    "#it takes the text and counts the probabilities for common words\n",
    "#and uses the frequentWordsCorpusMean and frequentWordsCorpusStdDev\n",
    "def calc_z_score(text,frequentWordsCorpusMean,frequentWordsCorpusStdDev):\n",
    "    word_counts=defaultdict(int)\n",
    "    totalWords=0\n",
    "    sentences=sent_tokenize(text.lower())\n",
    "    for sentence in sentences:\n",
    "        words=word_tokenize(sentence)\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        for word in frequentWordsCorpusMean:\n",
    "            if word in fdist:\n",
    "                word_counts[word]+=fdist[word]\n",
    "            else:\n",
    "                word_counts[word]+=0\n",
    "            totalWords+=fdist[word]\n",
    "    zScores={}\n",
    "    for word in word_counts:\n",
    "        word_dist=(word_counts[word]+0.000001)/(totalWords+0.000001)\n",
    "        zScores[word]=(word_dist-frequentWordsCorpusMean[word])/frequentWordsCorpusStdDev[word]\n",
    "    return zScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_email_match(text,frequentWordsCorpusMean,frequentWordsCorpusStdDev):\n",
    "    scores={}\n",
    "    min_score=1000000\n",
    "    min_name=''\n",
    "    zscores=calc_z_score(text,frequentWordsCorpusMean,frequentWordsCorpusStdDev)\n",
    "    for author in zScoresByAuthor:\n",
    "        score=0.0\n",
    "        for word in zScoresByAuthor[author]:\n",
    "            score+=abs(zscores[word]-zScoresByAuthor[author][word])\n",
    "        score/=len(zScoresByAuthor[author])\n",
    "        scores[author]=score\n",
    "        if score<min_score:\n",
    "            min_score=score\n",
    "            min_name=author\n",
    "    return min_name,min_score,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1000\n",
    "text=df_test.iloc[i]['FormattedMessage']\n",
    "name=df_test.iloc[i]['From']\n",
    "print(name)\n",
    "print(text)\n",
    "author,min_score,scores=find_email_match(text,frequentWordsCorpusMean,frequentWordsCorpusStdDev)\n",
    "print(author,min_score)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x=np.exp(x-np.max(x))\n",
    "    out=e_x/e_x.sum()\n",
    "    return out\n",
    "\n",
    "def cross_entropy_loss(predictions, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n",
    "    return ce\n",
    "\n",
    "def get_probs(text,encoded_classes,frequentWordsCorpusMean,frequentWordsCorpusStdDev):\n",
    "    zscores=calc_z_score(text,frequentWordsCorpusMean,frequentWordsCorpusStdDev)\n",
    "    returnMatrix=[0.0 for author in zScoresByAuthor]\n",
    "    for author in zScoresByAuthor:\n",
    "        score=0.0\n",
    "        for word in zScoresByAuthor[author]:\n",
    "            score+=abs(zscores[word]-zScoresByAuthor[author][word])\n",
    "        score/=len(zScoresByAuthor[author])\n",
    "        for i,item in enumerate(encoded_classes):\n",
    "            if item==author: returnMatrix[i]=score\n",
    "    returnMatrix=[-x for x in returnMatrix]\n",
    "    return softmax(returnMatrix),returnMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "enc=LabelBinarizer()\n",
    "enc.fit(filter_list)\n",
    "print(enc.classes_)\n",
    "all_text=df_test['FormattedMessage'].tolist()\n",
    "from_list=df_test['From'].tolist()\n",
    "y_values=enc.transform(from_list)\n",
    "y_pred=[]\n",
    "for text in all_text:\n",
    "    prob,blah=get_probs(text,enc.classes_,frequentWordsCorpusMean,frequentWordsCorpusStdDev)\n",
    "    y_pred.append(prob)\n",
    "    \n",
    "y_pred=np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "print(from_list[i])\n",
    "print(all_text[i])\n",
    "print(enc.inverse_transform(y_values)[i])\n",
    "print(y_pred[i])\n",
    "print(enc.classes_[np.argmax(y_pred[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_entropy_loss(y_values,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
